{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11453</th>\n",
       "      <td>3045897535.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1016334938.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>1848723275.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>1598014441.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14853</th>\n",
       "      <td>3664478040.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id  label\n",
       "11453  3045897535.jpg      3\n",
       "95     1016334938.jpg      3\n",
       "4761   1848723275.jpg      2\n",
       "3415   1598014441.jpg      3\n",
       "14853  3664478040.jpg      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_path = 'data/'\n",
    "df = pd.read_csv(general_path + 'train.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = df.to_numpy()\n",
    "image_list = train_list[:,0].astype(np.str)\n",
    "label_list = train_list[:,1].astype(np.uint8)\n",
    "data_list = np.column_stack((image_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(label_list, bins=np.arange(6)-0.5, density=False, cumulative=False, histtype='bar', rwidth=0.8)\n",
    "plt.xticks(range(5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_data(train_list):\n",
    "    '''\n",
    "        train_list[:][0] : image's file name\n",
    "        train_list[:][1] : image's label\n",
    "    '''\n",
    "    height = 456\n",
    "    width = 456\n",
    "    \n",
    "    length = train_list.shape[0]\n",
    "    ori_list = np.arange(length)\n",
    "    aug_list = np.random.choice(ori_list, length, replace=True)\n",
    "    \n",
    "    np.random.shuffle(ori_list)\n",
    "    np.random.shuffle(aug_list)\n",
    "    \n",
    "    zeros = np.zeros(length)\n",
    "    ones = np.ones(length)\n",
    "    is_aug = np.append(zeros, ones)\n",
    "    \n",
    "    total_list = np.append(ori_list, aug_list)\n",
    "    total_list = np.column_stack((total_list, is_aug)).astype(np.int32)\n",
    "    np.random.shuffle(total_list)\n",
    "    \n",
    "    for num, aug in total_list:\n",
    "        if aug == 0:\n",
    "            ori_img = tf.io.read_file(general_path + 'train_images/' + train_list[num][0].decode('utf-8'))\n",
    "            ori_img = tf.image.decode_jpeg(ori_img, channels=3)\n",
    "            img = tf.image.resize(ori_img, [456, 456])\n",
    "\n",
    "            label = int(train_list[num][1])\n",
    "            n_values = 5\n",
    "            one_hot = tf.eye(n_values)[label]\n",
    "\n",
    "            yield (tf.reshape(img, [1, height, width, 3]) / 255, tf.reshape(one_hot, [1, 5]))\n",
    "            \n",
    "        else:\n",
    "            \"\"\"\n",
    "                1) crop\n",
    "                2) rotate90\n",
    "                3) flip(horizon, vertical)\n",
    "                4) hue\n",
    "                5) brightness\n",
    "                6) constrast\n",
    "                7) clip\n",
    "            \"\"\"\n",
    "            rc = np.random.randint(0, 300)\n",
    "            rr = np.random.randint(0, 3)\n",
    "            \n",
    "            # load a image\n",
    "            ori_img = tf.io.read_file(general_path + 'train_images/' + train_list[num][0].decode('utf-8'))\n",
    "            \n",
    "            # decode a raw image\n",
    "            ori_img = tf.image.decode_jpeg(ori_img, channels=3)\n",
    "            \n",
    "            img_height, img_width, _ = ori_img.shape\n",
    "            \n",
    "            # 1) crop\n",
    "            img = tf.image.random_crop(ori_img, [img_height - rc, img_height - rc, 3])\n",
    "            \n",
    "            # 2) rotate90\n",
    "            img = tf.image.rot90(img, k=rr)\n",
    "            \n",
    "            # 3) flip(horizon, vertical)\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            \n",
    "            # 4) hue\n",
    "            img = tf.image.random_hue(img, 0.1)\n",
    "            \n",
    "            # 5) brightness\n",
    "            img = tf.image.random_brightness(img, 0.3)\n",
    "            \n",
    "            # 6) contrast\n",
    "            img = tf.image.random_contrast(img, 0.6, 1.8)\n",
    "            \n",
    "            # 7) clip\n",
    "            img = tf.clip_by_value(img, 0, 255)\n",
    "            \n",
    "            img = tf.image.resize(img, [456, 456])\n",
    "            \n",
    "            # one-hot encoding\n",
    "            label = int(train_list[num][1])\n",
    "            n_values = 5\n",
    "            one_hot = tf.eye(n_values)[label]\n",
    "\n",
    "            yield (tf.reshape(img, [1, height, width, 3]) / 255, tf.reshape(one_hot, [1, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_data_val(val_list):\n",
    "    height = 456\n",
    "    width = 456\n",
    "    \n",
    "    for image_name, label in val_list:\n",
    "        ori_img = tf.io.read_file(general_path + 'train_images/' + image_name.decode('utf-8'))\n",
    "        ori_img = tf.image.decode_jpeg(ori_img, channels=3)\n",
    "        img = tf.image.resize(ori_img, [456, 456])\n",
    "\n",
    "        label = int(label)\n",
    "        n_values = 5\n",
    "        one_hot = tf.eye(n_values)[label]\n",
    " \n",
    "        yield (tf.reshape(img, [1, height, width, 3]) / 255, tf.reshape(one_hot, [1, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Test code ***\n",
    "\n",
    "# rc = np.random.randint(0, 300)\n",
    "# rr = np.random.randint(0, 3)\n",
    "# ori_img = tf.io.read_file(general_path + 'train_images/' + '85453943.jpg')\n",
    "# ori_img = tf.image.decode_jpeg(ori_img, channels=3)\n",
    "# img_height, img_width, _ = ori_img.shape\n",
    "# img = tf.image.random_crop(ori_img, [img_height - rc, img_height - rc, 3])\n",
    "# img = tf.image.rot90(img, k=rr)\n",
    "# img = tf.image.random_flip_left_right(img)\n",
    "# img = tf.image.random_flip_up_down(img)\n",
    "# img = tf.image.random_hue(img, 0.1)\n",
    "# img = tf.image.random_brightness(img, 0.15)\n",
    "# img = tf.image.random_contrast(img, 0.8, 1.5)\n",
    "\n",
    "# img = tf.clip_by_value(img, 0, 255)\n",
    "# img = tf.image.resize(img, [456, 456])\n",
    "\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.axis('off')\n",
    "# plt.imshow(img / 255)\n",
    "# plt.title('augmented image')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.axis('off')\n",
    "# plt.imshow(ori_img)\n",
    "# plt.title('original image')\n",
    "\n",
    "# plt.show()\n",
    "# print(tf.math.reduce_max(augmented_img))\n",
    "# print(tf.math.reduce_min(augmented_img))\n",
    "# print(tf.math.maximum(tf.reshape(augmented_img, [456*456*3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(456, 456, 3))\n",
    "\n",
    "effnet = tf.keras.applications.EfficientNetB5(\n",
    "    include_top=False, weights='imagenet', input_tensor=None,\n",
    "    input_shape=None, pooling='avg', classes=5,\n",
    "    classifier_activation='softmax'\n",
    ")(inputs)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')(effnet)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(top_dropout_rate, name='top_dropout')(x)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='pred')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train_validation\n",
    "\n",
    "np.random.shuffle(data_list)\n",
    "split_ratio = int(data_list.shape[0]*0.8)\n",
    "train_list = data_list[:split_ratio]\n",
    "val_list = data_list[split_ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "\n",
    "checkpoint_path = \"data/checkpoint/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    period=1)\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_leaf_data,\n",
    "                                         output_types=(tf.float32, tf.float32),\n",
    "                                         output_shapes=([None, 456, 456, 3], [None, 5]),\n",
    "                                         args=(train_list, ))\n",
    "\n",
    "val = tf.data.Dataset.from_generator(get_leaf_data_val,\n",
    "                                         output_types=(tf.float32, tf.float32),\n",
    "                                         output_shapes=([None, 456, 456, 3], [None, 5]),\n",
    "                                         args=(val_list, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  34234/Unknown - 5797s 169ms/step - loss: 0.7328 - acc: 0.7294\n",
      "Epoch 00001: saving model to data/checkpoint\\cp-0001.ckpt\n",
      "34234/34234 [==============================] - 5969s 174ms/step - loss: 0.7328 - acc: 0.7294 - val_loss: 1.7435 - val_acc: 0.4951\n",
      "Epoch 2/20\n",
      "34234/34234 [==============================] - ETA: 0s - loss: 0.4758 - acc: 0.8305\n",
      "Epoch 00002: saving model to data/checkpoint\\cp-0002.ckpt\n",
      "34234/34234 [==============================] - 5970s 174ms/step - loss: 0.4758 - acc: 0.8305 - val_loss: 2.4087 - val_acc: 0.5626\n",
      "Epoch 3/20\n",
      "34234/34234 [==============================] - ETA: 0s - loss: 0.3996 - acc: 0.8601\n",
      "Epoch 00003: saving model to data/checkpoint\\cp-0003.ckpt\n",
      "34234/34234 [==============================] - 5913s 173ms/step - loss: 0.3996 - acc: 0.8601 - val_loss: 2.6122 - val_acc: 0.5699\n",
      "Epoch 4/20\n",
      "34234/34234 [==============================] - ETA: 0s - loss: 0.3506 - acc: 0.8794\n",
      "Epoch 00004: saving model to data/checkpoint\\cp-0004.ckpt\n",
      "34234/34234 [==============================] - 5929s 173ms/step - loss: 0.3506 - acc: 0.8794 - val_loss: 2.9374 - val_acc: 0.5930\n",
      "Epoch 5/20\n",
      "34234/34234 [==============================] - ETA: 0s - loss: 0.3160 - acc: 0.8929\n",
      "Epoch 00005: saving model to data/checkpoint\\cp-0005.ckpt\n",
      "34234/34234 [==============================] - 5931s 173ms/step - loss: 0.3160 - acc: 0.8929 - val_loss: 4.6804 - val_acc: 0.4005\n",
      "Epoch 6/20\n",
      "34234/34234 [==============================] - ETA: 0s - loss: 0.2984 - acc: 0.8997\n",
      "Epoch 00006: saving model to data/checkpoint\\cp-0006.ckpt\n",
      "34234/34234 [==============================] - 5943s 174ms/step - loss: 0.2984 - acc: 0.8997 - val_loss: 3.7402 - val_acc: 0.4173\n",
      "Epoch 7/20\n",
      "34234/34234 [==============================] - ETA: 0s - loss: 0.2745 - acc: 0.9098\n",
      "Epoch 00007: saving model to data/checkpoint\\cp-0007.ckpt\n",
      "34234/34234 [==============================] - 5987s 175ms/step - loss: 0.2745 - acc: 0.9098 - val_loss: 3.1925 - val_acc: 0.4734\n",
      "Epoch 8/20\n",
      "15059/34234 [============>.................] - ETA: 1:18:08 - loss: 0.2708 - acc: 0.9133"
     ]
    }
   ],
   "source": [
    "hist = model.fit(dataset, validation_data=val, callbacks=[cp_callback], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
